{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-CC-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCjk4TRQ1k_"
      },
      "source": [
        "Group 13: group_1_11_95\n",
        "\n",
        "Akash Tike (AU1741001)\\\n",
        "Smit Mandavia (AU1741011)\\\n",
        "Shaunak Vyas (AU1741095)\n",
        "\n",
        "Here is our implementation of Recurrent Neural Network for text profanity detection under the joint project of the courses Artificial Intelligence and Cloud Computing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rxA-L-DSqa5"
      },
      "source": [
        "# Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8YcV1WEQ0fk"
      },
      "source": [
        "# importting required library \n",
        "\n",
        "# numerical processing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "\n",
        "# language processing libraries\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import FreqDist\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# machine learning libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, auc, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# timer to keep track of process durations\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0hDk0VzVr7R"
      },
      "source": [
        "# installing required packages on new runtime\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8jzKvWCWmWb"
      },
      "source": [
        "# importing data from google drive instead of uploading, for faster access to files\n",
        "file_id = '1va-lNyLHRjBaUDIz8Bw2aKIInvNGOAf1'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloaded = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  _, done = downloader.next_chunk()\n",
        "\n",
        "downloaded.seek(0)\n",
        "data_file = pd.read_csv(downloaded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkiKPaFHXL8n"
      },
      "source": [
        "# train test splitting\n",
        "data_file.describe()\n",
        "target_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "# independent feature (input) is comments\n",
        "X = data_file['comment_text']\n",
        "# 6 dependent features (output), described by target_classes\n",
        "y = data_file[target_classes]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhuJM-2kmBeE"
      },
      "source": [
        "# language preprocessing \n",
        "# get the list of stopwords of english language, we should be handling the stop words beforehand\n",
        "stopwords_list = stopwords.words('english')\n",
        "\n",
        "# also, add the punctuation marks in stopwords_list \n",
        "stopwords_list += list(string.punctuation)\n",
        "stopwords_list += (\"''\",\"``\", \"'s\", \"\\\\n\\\\n\" , '...', 'i\\\\','\\\\n', '•', \"i\", 'the', \"'m\", 'i\\\\', \"'ve\", \"don\\\\'t\", \"'re\", \"\\\\n\\\\ni\", \"it\\\\\", \"'ll\", 'you\\\\', \"'d\", \"n't\", '’', 'article', 'page', 'wikipedia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omOU2HzwqNGs"
      },
      "source": [
        "# tokenize the words of training dataset\n",
        "train_text = str(list(X_train)).lower()\n",
        "tokens = word_tokenize(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zVyKtPesl4-"
      },
      "source": [
        "# lemmatize the text\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens =[lemmatizer.lemmatize(w) for w in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI_LrbLTtbJm"
      },
      "source": [
        "# importing keras library functions \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, LSTM, Embedding\n",
        "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpojtKfbutal"
      },
      "source": [
        "# initializing tokenizer\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npL98I7Xu2L4"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving the tokenizer to pickle file which will be later used on server\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF_0xlm8vg4p"
      },
      "source": [
        "# convert the text data to sequence\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
        "# after that we pad the sequence to make it of equal legth\n",
        "maxlen = 400\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUgiLlHawIWE"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# set any random seed value\n",
        "seed_val = 3\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(seed_val)\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "tf.random.set_seed(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDemdEjgxadm"
      },
      "source": [
        "!pip install focal-loss\n",
        "from focal_loss import BinaryFocalLoss \n",
        "\n",
        "early_stopping = [EarlyStopping(monitor='val_loss', patience=2), \n",
        "                  ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyP8eZBxmfF"
      },
      "source": [
        "rnn_with_lstm = Sequential()\n",
        "embedding_size = 128\n",
        "# embedding layer that maps features (input) to embedding size\n",
        "rnn_with_lstm.add(Embedding(max_features, embedding_size))\n",
        "# adding LSTM layer to retain the memory for longer period of time\n",
        "rnn_with_lstm.add(LSTM(60, return_sequences=True,name='lstm_layer'))        \n",
        "# globalmaxpooling for dimensionality reduction on max value\n",
        "rnn_with_lstm.add(GlobalMaxPool1D())\n",
        "# dropout layer to avoid overfitting\n",
        "rnn_with_lstm.add(Dropout(0.1))  \n",
        "# dense hidden layers of size 50 and 10 respectively, with same dropout\n",
        "rnn_with_lstm.add(Dense(50, kernel_regularizer=regularizers.l2(.00001),activation='relu'))\n",
        "rnn_with_lstm.add(Dropout(.01))\n",
        "rnn_with_lstm.add(Dense(10, kernel_regularizer=regularizers.l2(.00001),activation='relu'))\n",
        "# finally output layer, which is mapped to 6 different target classes\n",
        "rnn_with_lstm.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "# compile the rnn model\n",
        "rnn_with_lstm.compile(loss=BinaryFocalLoss(gamma=2), optimizer='adam', metrics=['accuracy'])\n",
        "# dimensions of each layer of rnn\n",
        "for layer in rnn_with_lstm.layers:\n",
        "    print(layer.name, \" \", layer.output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSRsNFTKzZe4"
      },
      "source": [
        "start = time.time()\n",
        "fit_rnn_model = rnn_with_lstm.fit(X_t, y_train, epochs=5, batch_size=400, \n",
        "                        callbacks=early_stopping, validation_split=0.3)\n",
        "end = time.time()\n",
        "print(f\"Total time taken to fit model: {end-start}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdzNx4jJzZcH"
      },
      "source": [
        "# save trained model and weights to file that will be used on server for prediction\n",
        "from keras.models import model_from_json\n",
        "model_json = rnn_with_lstm.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "rnn_with_lstm.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OIJi5Fl0x4w"
      },
      "source": [
        "# testing the model\n",
        "y_prediction = rnn_with_lstm.predict(X_te)\n",
        "y_prediction_binary = pd.DataFrame(np.round(y_prediction), columns=target_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P83e-bs0d70"
      },
      "source": [
        "mae = mean_absolute_error(y_true = y_test, y_pred = y_prediction_binary)\n",
        "print('Mean absolute error : ' , mae)\n",
        "mse = mean_squared_error(y_true = y_test, y_pred = y_prediction_binary)\n",
        "print('Mean squared error : ' , mse)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}